{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14481170,"datasetId":9249227,"databundleVersionId":15304666}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers==4.40.0\n!pip install -q epitran\n!pip install -q sacrebleu\n!pip install -q sentencepiece\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:01:29.048962Z","iopub.execute_input":"2026-02-21T16:01:29.049161Z","iopub.status.idle":"2026-02-21T16:01:56.622681Z","shell.execute_reply.started":"2026-02-21T16:01:29.049141Z","shell.execute_reply":"2026-02-21T16:01:56.621837Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.2.0 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom torch.optim import AdamW\nimport epitran\nimport json\nfrom tqdm import tqdm\nfrom sacrebleu import corpus_bleu\nfrom sklearn.model_selection import train_test_split\n\n# ============================================================\n# DEVICE\n# ============================================================\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"DEVICE:\", device)\n\n# ============================================================\n# LOAD DATA\n# ============================================================\n\nDATA_DIR = \"/kaggle/input/datasets/nooraliikhwan/decoderdatav3\"\njson_file = [f for f in os.listdir(DATA_DIR) if f.endswith(\".jsonl\")][0]\nDATA_PATH = os.path.join(DATA_DIR, json_file)\n\nraw_data = []\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        raw_data.append(json.loads(line)[\"text\"])\n\nprint(\"Original size:\", len(raw_data))\n\nclean_data = list(set(raw_data))\nprint(\"After dedup:\", len(clean_data))\n\ndef extract_parts(text):\n    try:\n        inp = text.split(\"Input:\")[1].split(\"Output:\")[0].strip()\n        out = text.split(\"Output:\")[1].split(\"<|end|>\")[0].strip()\n        return inp, out\n    except:\n        return None, None\n\nparsed_data = []\nfor t in clean_data:\n    inp, out = extract_parts(t)\n    if inp and out:\n        parsed_data.append((t, inp, out))\n\nprint(\"Valid samples:\", len(parsed_data))\n\ntrain_data, val_data = train_test_split(parsed_data, test_size=0.1, random_state=42)\nprint(\"Train:\", len(train_data))\nprint(\"Val:\", len(val_data))\n\n# ============================================================\n# ENCODERS\n# ============================================================\n\nclass MorphologyEncoder(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.char_emb = nn.Embedding(128, 64)\n        self.proj = nn.Sequential(\n            nn.Linear(64, hidden),\n            nn.LayerNorm(hidden)\n        )\n\n    def forward(self, texts):\n        vecs = []\n        for t in texts:\n            chars = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            chars += [0] * (64 - len(chars))\n            chars = torch.tensor(chars, device=device)\n            emb = self.char_emb(chars).mean(0)\n            vecs.append(emb)\n        return self.proj(torch.stack(vecs))\n\n\nclass PhonemeEncoder(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.epi = epitran.Epitran('msa-Latn')\n        self.map = {p: i for i, p in enumerate(\"pbtdkgmnŋshlrwjiɛaəɔou\")}\n        self.emb = nn.Embedding(len(self.map) + 1, 64)\n        self.proj = nn.Sequential(\n            nn.Linear(64, hidden),\n            nn.LayerNorm(hidden)\n        )\n\n    def forward(self, texts):\n        vecs = []\n        for t in texts:\n            try:\n                ph = [p for p in self.epi.transliterate(t) if p in self.map]\n            except:\n                ph = []\n            ids = [self.map.get(p, len(self.map)) for p in ph[:64]]\n            ids += [len(self.map)] * (64 - len(ids))\n            ids = torch.tensor(ids, device=device)\n            emb = self.emb(ids).mean(0)\n            vecs.append(emb)\n        return self.proj(torch.stack(vecs))\n\n\nclass SAT(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(hidden, 8, batch_first=True)\n\n    def forward(self, h):\n        out, _ = self.attn(h, h, h)\n        return out.mean(1)\n\n# ============================================================\n# FUSION GPT\n# ============================================================\n\nclass FusionGPT(nn.Module):\n    def __init__(self, use_morph=False, use_phon=False, use_sat=False):\n        super().__init__()\n\n        self.use_morph = use_morph\n        self.use_phon = use_phon\n        self.use_sat = use_sat\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n        hidden = self.gpt.config.hidden_size\n\n        if use_morph:\n            self.morph = MorphologyEncoder(hidden)\n            self.gm = nn.Linear(hidden, hidden)\n\n        if use_phon:\n            self.phon = PhonemeEncoder(hidden)\n            self.gp = nn.Linear(hidden, hidden)\n\n        if use_sat:\n            self.sat = SAT(hidden)\n            self.gs = nn.Linear(hidden, hidden)\n\n        self.sig = nn.Sigmoid()\n\n    def forward(self, texts, input_ids, attention_mask, labels=None):\n\n        outputs = self.gpt.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        h = outputs.last_hidden_state\n\n        if self.use_morph:\n            hm = self.morph(texts).unsqueeze(1)\n            h = h + 0.1 * self.sig(self.gm(hm)) * hm\n\n        if self.use_phon:\n            hp = self.phon(texts).unsqueeze(1)\n            h = h + 0.1 * self.sig(self.gp(hp)) * hp\n\n        if self.use_sat:\n            hs = self.sat(h).unsqueeze(1)\n            h = h + 0.1 * self.sig(self.gs(hs)) * hs\n\n        logits = self.gpt.lm_head(h)\n\n        loss = None\n        if labels is not None:\n            shift_logits = logits[:, :-1, :].contiguous()\n            shift_labels = labels[:, 1:].contiguous()\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n        return loss, logits\n\n# ============================================================\n# TRAIN + EVAL\n# ============================================================\n\ndef train_and_eval(name, use_morph=False, use_phon=False, use_sat=False):\n\n    print(\"\\n========\", name, \"========\")\n\n    model = FusionGPT(use_morph, use_phon, use_sat).to(device)\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n\n    loader = DataLoader(train_data, batch_size=4, shuffle=True)\n\n    # TRAIN\n    for epoch in range(5):\n        model.train()\n        total = 0\n\n        for batch in tqdm(loader):\n            texts = [b[0] for b in batch]\n\n            enc = model.tokenizer(\n                texts,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=128\n            ).to(device)\n\n            labels = enc.input_ids.clone()\n\n            # DYNAMIC MASKING\n            for i, text in enumerate(texts):\n                prefix = text.split(\"Output:\")[0] + \"Output:\"\n                prefix_ids = model.tokenizer(prefix, return_tensors=\"pt\").input_ids[0]\n                labels[i, :len(prefix_ids)] = -100\n\n            loss, _ = model(\n                texts,\n                enc.input_ids,\n                enc.attention_mask,\n                labels\n            )\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total += loss.item()\n\n        print(\"Epoch\", epoch + 1, \"Loss:\", total / len(loader))\n\n    # EVALUATION\n    model.eval()\n    preds = []\n    refs = []\n\n    for sample in val_data[:300]:\n        full_text, inp, ref = sample\n\n        prompt = f\"<|task:translate|>\\nInput: {inp}\\nOutput:\"\n        enc = model.tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        generated = enc.input_ids\n\n        for _ in range(50):\n\n            with torch.no_grad():\n                _, logits = model(\n                    [prompt],\n                    generated,\n                    torch.ones_like(generated),\n                    labels=None\n                )\n\n            next_token_logits = logits[:, -1, :]\n\n            # REPETITION PENALTY\n            for token_id in set(generated[0].tolist()):\n                next_token_logits[:, token_id] /= 1.2\n\n            probs = torch.softmax(next_token_logits / 0.9, dim=-1)\n            next_token = torch.multinomial(probs, 1)\n\n            generated = torch.cat([generated, next_token], dim=1)\n\n            if generated.size(1) > 10 and next_token.item() == model.tokenizer.eos_token_id:\n                break\n\n        gen_text = model.tokenizer.decode(generated[0], skip_special_tokens=True)\n        gen_out = gen_text.split(\"Output:\")[-1].strip()\n\n        preds.append(gen_out)\n        refs.append([ref])\n\n    bleu = corpus_bleu(preds, refs)\n    print(\"BLEU:\", bleu.score)\n\n    return bleu.score\n\n# ============================================================\n# RUN ABLATION\n# ============================================================\n\nbleu0 = train_and_eval(\"GPT Only\")\nbleu1 = train_and_eval(\"GPT + Morph\", use_morph=True)\nbleu2 = train_and_eval(\"GPT + Morph + Phon\", use_morph=True, use_phon=True)\nbleu3 = train_and_eval(\"GPT + Morph + Phon + SAT\", use_morph=True, use_phon=True, use_sat=True)\n\nprint(\"\\nFINAL RESULTS\")\nprint(\"GPT Only:\", bleu0)\nprint(\"Morph:\", bleu1)\nprint(\"Morph+Phon:\", bleu2)\nprint(\"Morph+Phon+SAT:\", bleu3)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:01:59.671033Z","iopub.execute_input":"2026-02-21T16:01:59.671593Z","iopub.status.idle":"2026-02-21T17:10:58.800520Z","shell.execute_reply.started":"2026-02-21T16:01:59.671558Z","shell.execute_reply":"2026-02-21T17:10:58.799790Z"}},"outputs":[{"name":"stdout","text":"DEVICE: cuda\nOriginal size: 7542\nAfter dedup: 6873\nValid samples: 6873\nTrain: 6185\nVal: 688\n\n======== GPT Only ========\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb82b825478747a5aa25e60a79eea5f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b08024ac6c443e3a6c9a494b8866630"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8075959473c942e597d9343fddf0c470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171a8a36840646aa9b13de00a63bc5ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b3a95c25c84dcfb6e32d58a41278be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72a7ba5288a4fb78c8bd7a425c3a7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c64d4332a57b483989023412e31e37b9"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:46<00:00,  9.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.394859467768344\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:49<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.29641955190405317\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:49<00:00,  9.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.27154170537867683\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:49<00:00,  9.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.25101167553367243\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:49<00:00,  9.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.2308888255209742\nBLEU: 21.092072484361665\n\n======== GPT + Morph ========\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n100%|██████████| 1547/1547 [02:51<00:00,  9.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.3966088924597908\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:51<00:00,  9.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.30853077114426364\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:52<00:00,  8.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.2779496340005614\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:50<00:00,  9.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.2558851892765372\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:50<00:00,  9.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.2322999156356287\nBLEU: 46.09625517245935\n\n======== GPT + Morph + Phon ========\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:53<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.3960601519815095\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:53<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.3053367976707503\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:54<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.27715783907921954\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:53<00:00,  8.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.24989427939747785\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:54<00:00,  8.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.23586854320475031\nBLEU: 27.88241097922203\n\n======== GPT + Morph + Phon + SAT ========\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n100%|██████████| 1547/1547 [02:58<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.3810774414836635\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:57<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.30331054755288483\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:58<00:00,  8.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.2821989989659846\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:57<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.24690632724533002\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1547/1547 [02:57<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.23128287373657314\nBLEU: 19.082372394933696\n\nFINAL RESULTS\nGPT Only: 21.092072484361665\nMorph: 46.09625517245935\nMorph+Phon: 27.88241097922203\nMorph+Phon+SAT: 19.082372394933696\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}